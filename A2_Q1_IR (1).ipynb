{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVnl5QwHnZyR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf243025",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os \n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.corpus import stopwords  \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "lemmatizer = WordNetLemmatizer() \n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = set(stopwords.words('english')) \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import string\n",
        "from re import search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h2oqqdHsatC"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "#files=os.listdir('/content/gdrive/MyDrive/IR Assignments/IR Assignment 1/Humor,Hist,Media,Food')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1e413955",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#Dataset access and path setting\n",
        "dir=\"/content/gdrive/MyDrive/IR Assignments/IR Assignment 1/Humor,Hist,Media,Food\"\n",
        "list_of_files = os.listdir(dir)\n",
        "\n",
        "files_path=[]\n",
        "for f in list_of_files:\n",
        "    files_path.append(dir+'/'+f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_hkO98otHMi"
      },
      "outputs": [],
      "source": [
        "len(files_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ec11f1e8"
      },
      "outputs": [],
      "source": [
        "#data pre-processing for Files\n",
        "def pre_processing(content):\n",
        "    preprocessed_data=[]\n",
        "    lem_words=[]\n",
        "    #removal of digits\n",
        "    result = re.sub(r'\\d+', '', content)\n",
        "\n",
        "    #punctuation removal\n",
        "    #result = result.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "    result=re.sub(r'[^\\w\\s]',' ',result)\n",
        "\n",
        "    #removal of whitespace\n",
        "    result = result.strip()\n",
        "\n",
        "    #to lowercase\n",
        "    result = result.lower()\n",
        "    \n",
        "    #result=re.sub(r'[^\\w\\s]',' ',result)\n",
        "\n",
        "    #Lemmatization\n",
        "    result = word_tokenize(result)\n",
        "    for word in result:\n",
        "            lem_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    #stopwords removal\n",
        "    content_sw_removed = [w for w in lem_words if not w in english_stopwords]\n",
        "    preprocessed_data.append(i)\n",
        "    preprocessed_data.append(f)\n",
        "    preprocessed_data.append(content_sw_removed)\n",
        "    \n",
        "    return preprocessed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk5JRqnmZ3vQ"
      },
      "outputs": [],
      "source": [
        "#data pre-processing for Query\n",
        "def pre_processing_query(content):\n",
        "    preprocessed_data=[]\n",
        "    lem_words=[]\n",
        "    #removal of digits\n",
        "    result = re.sub(r'\\d+', '', content)\n",
        "\n",
        "    #punctuation removal\n",
        "    #result = result.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "    result=re.sub(r'[^\\w\\s]',' ',result)\n",
        "\n",
        "    #removal of whitespace\n",
        "    result = result.strip()\n",
        "\n",
        "    #to lowercase\n",
        "    result = result.lower()\n",
        "    \n",
        "    #result=re.sub(r'[^\\w\\s]',' ',result)\n",
        "\n",
        "    #Lemmatization\n",
        "    result = word_tokenize(result)\n",
        "    for word in result:\n",
        "            lem_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "    #stopwords removal\n",
        "    content_sw_removed = [w for w in lem_words if not w in english_stopwords]\n",
        "    #preprocessed_data.append(i)\n",
        "    #preprocessed_data.append(f)\n",
        "    preprocessed_data.append(content_sw_removed)\n",
        "    \n",
        "    return content_sw_removed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44a6b210"
      },
      "outputs": [],
      "source": [
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "datalist=[]\n",
        "i=0\n",
        "doc_id_name_mapping={}\n",
        "for f in list_of_files:\n",
        "    with open(dir+'/'+f, 'r',errors='ignore',encoding='latin-1') as file:              \n",
        "            content = file.read().replace('\\n', '')\n",
        "            datalist.append(pre_processing(content))\n",
        "            doc_id_name_mapping[i]=f\n",
        "            i=i+1       \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6a689a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.DataFrame(datalist, columns =['Doc_Id', 'Doc_Name','Doc_Content'])\n",
        "all_doc_id=df['Doc_Id']\n",
        "\n",
        "collection_of_words=[]\n",
        "for i in range(len(df)) :\n",
        "    document_id=df.loc[i, \"Doc_Id\"]\n",
        "    document_content = df.loc[i, \"Doc_Content\"]\n",
        "    for word in document_content:\n",
        "        word_data=[]\n",
        "        word_data.append(word)\n",
        "        word_data.append(document_id)\n",
        "        collection_of_words.append(word_data)\n",
        "        \n",
        "collection_df = pd.DataFrame(collection_of_words,columns=('word','doc_id'))\n",
        "collection_df=collection_df.sort_values(by=['word','doc_id'])      \n",
        "collection_df=collection_df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rsadyhNuMxQ"
      },
      "outputs": [],
      "source": [
        "def intersection(lista, listb):\n",
        "    listc = [value for value in lista if value in listb]\n",
        "    listc=list(set(listc))\n",
        "    return listc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfNkCBoNuPy5"
      },
      "outputs": [],
      "source": [
        "def Union(lista, listb):\n",
        "    listc = sorted(lista + listb)\n",
        "    listc=list(set(listc))\n",
        "    return listc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yh0ZYcn0v9I4"
      },
      "outputs": [],
      "source": [
        "# Taking input of query\n",
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "query=input(\"Enter the query:\")\n",
        "#preprocessedquery=preprocessing(query)\n",
        "q_input=[]\n",
        "q_input=pre_processing_query(query)\n",
        "\n",
        "\n",
        "all_doc_id=df['Doc_Id']\n",
        "intersection_list=[]\n",
        "union_list=[]\n",
        "Jaccard_list1=[]\n",
        "Jaccard_list=[]\n",
        "collection_of_words=[]\n",
        "for i in range(len(df)) :\n",
        "  Jaccard_list1=[]\n",
        "  document_id=df.loc[i, \"Doc_Id\"]\n",
        "  document_name = df.loc[i, \"Doc_Name\"]\n",
        "  intersection_list=intersection(q_input,df.iloc[i][2])\n",
        "  union_list=Union(q_input,df.iloc[i][2])\n",
        "  jac_coff=len(intersection_list)/len(union_list)\n",
        "  Jaccard_list1.append(document_name)\n",
        "  Jaccard_list1.append(jac_coff)\n",
        "  Jaccard_list.append(Jaccard_list1)\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltf2bJqDrRdW"
      },
      "outputs": [],
      "source": [
        "Jaccard_list = pd.DataFrame(Jaccard_list, columns =['Doc_Name', 'Jac_Coff'])\n",
        "Jaccard_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SjdtHgSwsUx"
      },
      "outputs": [],
      "source": [
        "Jaccard_list=Jaccard_list.sort_values(by=\"Jac_Coff\",ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvdTZ9Qexf5n"
      },
      "outputs": [],
      "source": [
        "Jaccard_list.to_csv('/content/gdrive/MyDrive/IR Assignments/IR Assignment 1/Jaccard Coffecient.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TpNhHbsqTO2"
      },
      "outputs": [],
      "source": [
        "print(\"Document Name\", \" \", \"Jaccard Coefficient\")\n",
        "for i in range(5):\n",
        "  print(Jaccard_list.iloc[i][0], \" \", Jaccard_list.iloc[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmPC36CDyT85"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_zpRsOcyVqy"
      },
      "source": [
        "## **Q1 Part B**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsJe-np0yfXz"
      },
      "outputs": [],
      "source": [
        "word_freq = {} \n",
        "for i in range(len(df)):\n",
        "  for word in df.iloc[i][2]:\n",
        "    if word in word_freq:\n",
        "      word_freq[word].add(i)\n",
        "    else:\n",
        "      word_freq[word] = {i}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhkblQfjznOi"
      },
      "outputs": [],
      "source": [
        "len(word_freq.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKOOyRl-1Ftk"
      },
      "source": [
        "**IDF Calculation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aBIV6XZzo1E"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "IDF = []\n",
        "for word in word_freq.keys():\n",
        "  IDF.append(math.log(len(df)/(len(word_freq[word])+1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8C8cegnz0jZ"
      },
      "outputs": [],
      "source": [
        "len(IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNHWuu4y3cCf"
      },
      "outputs": [],
      "source": [
        "\n",
        "x=[0]*3\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDfw7Q6E1y9s"
      },
      "outputs": [],
      "source": [
        "def Binary(TF_IDF):\n",
        "  for i in range(len(df)):\n",
        "    word_count=[0]*len(word_freq.keys())\n",
        "    TF_IDF.append(word_count)\n",
        "  i = 0\n",
        "\n",
        "  for word in word_freq.keys():\n",
        "    lst_doc = word_freq[word]\n",
        "    for k in lst_doc:\n",
        "      TF_IDF[k][i] = 1\n",
        "    i= i+1\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(word_freq.keys())):\n",
        "      TF_IDF[i][j]= TF_IDF[i][j]*IDF[j]\n",
        "  return TF_IDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB32n3Zc5rCt"
      },
      "outputs": [],
      "source": [
        "TF_IDF=[]\n",
        "TF_IDF=Binary(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igKpZUmb8G3f"
      },
      "outputs": [],
      "source": [
        "len(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEXiMIJP548T"
      },
      "outputs": [],
      "source": [
        "TF_IDF = pd.DataFrame(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoSv7O1-6OY5"
      },
      "outputs": [],
      "source": [
        "def Raw_Count(TF_IDF): \n",
        "  for i in range(len(df)):\n",
        "    TF_IDF.append(0)\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    word_count = {}\n",
        "    for word in word_freq.keys():\n",
        "      word_count[word] = 0\n",
        "    \n",
        "    for word in df.iloc[i][2]:\n",
        "      word_count[word] = word_count[word]+1\n",
        "    TF_IDF[i] = list(word_count.values())\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(word_freq.keys())):\n",
        "      TF_IDF[i][j] = TF_IDF[i][j]*IDF[j]\n",
        "    \n",
        "  return TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iauQxbc_832U"
      },
      "outputs": [],
      "source": [
        "TF_IDF=[]\n",
        "TF_IDF=Raw_Count(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0-5_PY2_C3C"
      },
      "outputs": [],
      "source": [
        "def Term_Frequency(TF_IDF): \n",
        "  for i in range(len(df)):\n",
        "    TF_IDF.append(0)\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    word_count = {}\n",
        "    for word in word_freq.keys():\n",
        "      word_count[word] = 0\n",
        "    \n",
        "    for word in df.iloc[i][2]:\n",
        "      word_count[word] = word_count[word]+1\n",
        "\n",
        "    for word in word_count.keys():\n",
        "      word_count[word] = word_count[word]/len(df.iloc[i][2])\n",
        "\n",
        "    TF_IDF[i] = list(word_count.values())\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(word_freq.keys())):\n",
        "      TF_IDF[i][j] = TF_IDF[i][j]*IDF[j]\n",
        "    \n",
        "  return TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MuIc7Uf5Bq4E"
      },
      "outputs": [],
      "source": [
        "TF_IDF=[]\n",
        "TF_IDF=Term_Frequency(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Dr6L_y7SBu6e"
      },
      "outputs": [],
      "source": [
        "TF_IDF = pd.DataFrame(TF_IDF)\n",
        "TF_IDF.to_csv('/content/gdrive/MyDrive/IR Assignments/IR Assignment 1/Term Frequency TF_IDF.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MZqCW0e2B7LT"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def Log_Normalization(TF_IDF): \n",
        "  for i in range(len(df)):\n",
        "    TF_IDF.append(0)\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    word_count = {}\n",
        "    for word in word_freq.keys():\n",
        "      word_count[word] = 0\n",
        "    \n",
        "    for word in df.iloc[i][2]:\n",
        "      word_count[word] = word_count[word]+1\n",
        "\n",
        "    for word in word_count.keys():\n",
        "      word_count[word] = math.log(1 + word_count[word])\n",
        "\n",
        "    TF_IDF[i] = list(word_count.values())\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(word_freq.keys())):\n",
        "      TF_IDF[i][j] = TF_IDF[i][j]*IDF[j]\n",
        "    \n",
        "  return TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zOnSRzHcCvVV"
      },
      "outputs": [],
      "source": [
        "TF_IDF=[]\n",
        "TF_IDF=Log_Normalization(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gp3AXgjEC2aT"
      },
      "outputs": [],
      "source": [
        "TF_IDF = pd.DataFrame(TF_IDF)\n",
        "TF_IDF.to_csv('/content/gdrive/MyDrive/IR Assignments/IR Assignment 1/Log Normalization TF_IDF.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w0AbgsSYDNTz"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def Double_Normalization(TF_IDF): \n",
        "  for i in range(len(df)):\n",
        "    TF_IDF.append(0)\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    Maximum=0\n",
        "    word_count = {}\n",
        "    for word in word_freq.keys():\n",
        "      word_count[word] = 0\n",
        "    \n",
        "    for word in df.iloc[i][2]:\n",
        "      word_count[word] = word_count[word]+1\n",
        "    \n",
        "\n",
        "    for word in word_count.keys():\n",
        "      if(word_count[word]>Maximum):\n",
        "        Maximum = word_count[word]\n",
        "    \n",
        "    for word in word_count.keys():\n",
        "      word_count[word] = 0.5 + 0.5*(word_count[word]/Maximum)\n",
        "      \n",
        "    TF_IDF[i] = list(word_count.values())\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    for j in range(len(word_freq.keys())):\n",
        "      TF_IDF[i][j] = TF_IDF[i][j]*IDF[j]\n",
        "    \n",
        "  return TF_IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHT-Mm2O8L0_"
      },
      "outputs": [],
      "source": [
        "TF_IDF=[]\n",
        "TF_IDF=Double_Normalization(TF_IDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPxuVsFK8Oz9"
      },
      "outputs": [],
      "source": [
        "TF_IDF = pd.DataFrame(TF_IDF)\n",
        "TF_IDF.to_csv('/content/gdrive/MyDrive/IR Assignments/IR Assignment 1/Log Normalization TF_IDF.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vmj1PAUBEuay"
      },
      "outputs": [],
      "source": [
        "index = {}\n",
        "i=0\n",
        "for word in word_freq.keys():\n",
        "  index[word] = i\n",
        "  i = i+1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r395wdS9Ku_x"
      },
      "outputs": [],
      "source": [
        "def calc_top5(q_input, TF_IDF):\n",
        "  TF_IDF_pred=[]\n",
        "  TF_IDF_temp=[]\n",
        "  for i in range(len(df)):\n",
        "    TF_IDF_temp=[]\n",
        "    score=0\n",
        "    for word in q_input:\n",
        "      score=score+ TF_IDF[i][index[word]]\n",
        "    TF_IDF_temp.append(df.iloc[i][1])\n",
        "    TF_IDF_temp.append(score)\n",
        "    TF_IDF_pred.append(TF_IDF_temp)\n",
        "  return TF_IDF_pred\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlVRMv3RJgIr"
      },
      "outputs": [],
      "source": [
        "# Taking input of query\n",
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "query=input(\"Enter the query:\")\n",
        "#preprocessedquery=preprocessing(query)\n",
        "q_input=[]\n",
        "q_input=pre_processing_query(query)\n",
        "TF_IDF=[]\n",
        "TF_IDF=Binary(TF_IDF)\n",
        "TF_IDF_Prediction=[]\n",
        "TF_IDF_Prediction= calc_top5(q_input,TF_IDF)\n",
        "\n",
        "TF_IDF_Prediction = pd.DataFrame(TF_IDF_Prediction, columns =['Document Name', 'TF IDF Score'])\n",
        "\n",
        "TF_IDF_Prediction=TF_IDF_Prediction.sort_values(by=\"TF IDF Score\",ascending=False)\n",
        "print(\"Top 5 Documents using Binary weigthing Scheme \\n\")\n",
        "print(\"Document Name\", \" \", \"TF IDF Score\")\n",
        "for i in range(5):\n",
        "  print(TF_IDF_Prediction.iloc[i][0], \" \", TF_IDF_Prediction.iloc[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXmDQA0fNVzZ"
      },
      "outputs": [],
      "source": [
        "# Taking input of query\n",
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "query=input(\"Enter the query:\")\n",
        "#preprocessedquery=preprocessing(query)\n",
        "q_input=[]\n",
        "q_input=pre_processing_query(query)\n",
        "TF_IDF=[]\n",
        "TF_IDF=Raw_Count(TF_IDF)\n",
        "TF_IDF_Prediction=[]\n",
        "TF_IDF_Prediction= calc_top5(q_input,TF_IDF)\n",
        "\n",
        "TF_IDF_Prediction = pd.DataFrame(TF_IDF_Prediction, columns =['Document Name', 'TF IDF Score'])\n",
        "\n",
        "TF_IDF_Prediction=TF_IDF_Prediction.sort_values(by=\"TF IDF Score\",ascending=False)\n",
        "print(\"Top 5 Documents using Raw Count weigthing Scheme \\n\")\n",
        "print(\"Document Name\", \" \", \"TF IDF Score\")\n",
        "for i in range(5):\n",
        "  print(TF_IDF_Prediction.iloc[i][0], \" \", TF_IDF_Prediction.iloc[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ3xwtzoJZrO"
      },
      "outputs": [],
      "source": [
        "# Taking input of query\n",
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "query=input(\"Enter the query:\")\n",
        "#preprocessedquery=preprocessing(query)\n",
        "q_input=[]\n",
        "q_input=pre_processing_query(query)\n",
        "TF_IDF=[]\n",
        "TF_IDF=Term_Frequency(TF_IDF)\n",
        "TF_IDF_Prediction=[]\n",
        "TF_IDF_Prediction= calc_top5(q_input,TF_IDF)\n",
        "\n",
        "TF_IDF_Prediction = pd.DataFrame(TF_IDF_Prediction, columns =['Document Name', 'TF IDF Score'])\n",
        "\n",
        "TF_IDF_Prediction=TF_IDF_Prediction.sort_values(by=\"TF IDF Score\",ascending=False)\n",
        "print(\"Top 5 Documents using Term Frequency weigthing Scheme \\n\")\n",
        "print(\"Document Name\", \" \", \"TF IDF Score\")\n",
        "for i in range(5):\n",
        "  print(TF_IDF_Prediction.iloc[i][0], \" \", TF_IDF_Prediction.iloc[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flEmwGUoPaVz"
      },
      "outputs": [],
      "source": [
        "# Taking input of query\n",
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "query=input(\"Enter the query:\")\n",
        "#preprocessedquery=preprocessing(query)\n",
        "q_input=[]\n",
        "q_input=pre_processing_query(query)\n",
        "TF_IDF=[]\n",
        "TF_IDF=Log_Normalization(TF_IDF)\n",
        "TF_IDF_Prediction=[]\n",
        "TF_IDF_Prediction= calc_top5(q_input,TF_IDF)\n",
        "\n",
        "TF_IDF_Prediction = pd.DataFrame(TF_IDF_Prediction, columns =['Document Name', 'TF IDF Score'])\n",
        "\n",
        "TF_IDF_Prediction=TF_IDF_Prediction.sort_values(by=\"TF IDF Score\",ascending=False)\n",
        "print(\"Top 5 Documents using Log Normalization weigthing Scheme \\n\")\n",
        "print(\"Document Name\", \" \", \"TF IDF Score\")\n",
        "for i in range(5):\n",
        "  print(TF_IDF_Prediction.iloc[i][0], \" \", TF_IDF_Prediction.iloc[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvDoniUsPaRz"
      },
      "outputs": [],
      "source": [
        "# Taking input of query\n",
        "#preprocession of each file in Dataset\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('wordnet')\n",
        "query=input(\"Enter the query:\")\n",
        "#preprocessedquery=preprocessing(query)\n",
        "q_input=[]\n",
        "q_input=pre_processing_query(query)\n",
        "TF_IDF=[]\n",
        "TF_IDF=Double_Normalization(TF_IDF)\n",
        "TF_IDF_Prediction=[]\n",
        "TF_IDF_Prediction= calc_top5(q_input,TF_IDF)\n",
        "\n",
        "TF_IDF_Prediction = pd.DataFrame(TF_IDF_Prediction, columns =['Document Name', 'TF IDF Score'])\n",
        "\n",
        "TF_IDF_Prediction=TF_IDF_Prediction.sort_values(by=\"TF IDF Score\",ascending=False)\n",
        "print(\"Top 5 Documents using Double Normalization weigthing Scheme \\n\")\n",
        "print(\"Document Name\", \" \", \"TF IDF Score\")\n",
        "for i in range(5):\n",
        "  print(TF_IDF_Prediction.iloc[i][0], \" \", TF_IDF_Prediction.iloc[i][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn8UYSLOPaOH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7y3IK8ePaJF"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLJ-1B76PaGI"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "A2_Q1_IR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}